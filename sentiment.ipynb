{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ".. module:: SentimentTwAir\n",
    "\n",
    "SentimentTwAir\n",
    "*************\n",
    "\n",
    ":Description: SentimentTwAir\n",
    "\n",
    "\n",
    ":Version: \n",
    "\n",
    ":Created on: 07/09/2017 9:05 \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam, Adamax\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from collections import Counter\n",
    "import argparse\n",
    "import time\n",
    "import sklearn\n",
    "\n",
    "def tweet_to_words(raw_tweet):\n",
    "    \"\"\"\n",
    "    Only keeps ascii characters in the tweet and discards @words\n",
    "\n",
    "    :param raw_tweet:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z@]\", \" \", raw_tweet)\n",
    "    words = letters_only.lower().split()\n",
    "    meaningful_words = [w for w in words if not re.match(\"^[@]\", w)]\n",
    "    return \" \".join(meaningful_words)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--verbose', help=\"Verbose output (enables Keras verbose output)\", action='store_true', default=True)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "verbose = 1 if args.verbose else 0\n",
    "impl = 2\n",
    "\n",
    "print(\"Starting:\", time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''############################################\n",
    "# Data\n",
    "\n",
    "#    Tweet = pandas.read_csv(\"Airlines.csv\")\n",
    "#    Tweet = pandas.read_csv(\"Presidential.csv\")\n",
    "Tweet = pandas.read_csv(\"amazon_alexa.tsv\", sep='\\t')\n",
    "#Tweet = pandas.read_csv(\"kindle_reviews.csv\")\n",
    "\n",
    "# Shuffle el dataset\n",
    "Tweet = sklearn.utils.shuffle(Tweet)\n",
    "\n",
    "# El numero de rows que queremos que tenga el dataset\n",
    "#df_num_of_rows = int(Tweet.shape[0] / 320)\n",
    "#df_num_of_rows = df_num_of_rows + df_num_of_rows\n",
    "\n",
    "#Tweet = Tweet.iloc[:df_num_of_rows,:]\n",
    "\n",
    "# Pre-process the tweet and store in a separate column\n",
    "#Tweet['clean_tweet'] = Tweet['reviewText'].apply(lambda x: tweet_to_words(str(x)))\n",
    "Tweet['clean_tweet'] = Tweet['verified_reviews'].apply(lambda x: tweet_to_words(str(x)))\n",
    "# Convert sentiment to binary\n",
    "#Tweet['sentiment'] = Tweet['overall'].apply(lambda x: x - 1)\n",
    "Tweet['sentiment'] = Tweet['feedback'].apply(lambda x: x)\n",
    "\n",
    "# Join all the words in review to build a corpus\n",
    "all_text = ' '.join(Tweet['clean_tweet'])\n",
    "words = all_text.split()\n",
    "\n",
    "# Convert words to integers\n",
    "counts = Counter(words)\n",
    "\n",
    "numwords = 200  # Limit the number of words to use\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)[:numwords]\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "tweet_ints = []\n",
    "for each in Tweet['clean_tweet']:\n",
    "    tweet_ints.append([vocab_to_int[word] for word in each.split() if word in vocab_to_int])\n",
    "\n",
    "# Create a list of labels\n",
    "labels = np.array(Tweet['sentiment'])\n",
    "\n",
    "# Find the number of tweets with zero length after the data pre-processing\n",
    "tweet_len = Counter([len(x) for x in tweet_ints])\n",
    "print(\"Zero-length reviews: {}\".format(tweet_len[0]))\n",
    "print(\"Maximum tweet length: {}\".format(max(tweet_len)))\n",
    "\n",
    "# Remove those tweets with zero length and its corresponding label\n",
    "tweet_idx = [idx for idx, tweet in enumerate(tweet_ints) if len(tweet) > 0]\n",
    "labels = labels[tweet_idx]\n",
    "Tweet = Tweet.iloc[tweet_idx]\n",
    "tweet_ints = [tweet for tweet in tweet_ints if len(tweet) > 0]\n",
    "\n",
    "seq_len = max(tweet_len)\n",
    "features = np.zeros((len(tweet_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(tweet_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "\n",
    "split_frac = 0.8\n",
    "split_idx = int(len(features) * 0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x) * 0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "        \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "        \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "\n",
    "print(\"Train set: \\t\\t{}\".format(train_y.shape),\n",
    "        \"\\nValidation set: \\t{}\".format(val_y.shape),\n",
    "        \"\\nTest set: \\t\\t{}\".format(test_y.shape))\n",
    "\n",
    "# Save dataset preprocessed\n",
    "np.save(\"train_x\", train_x)\n",
    "np.save(\"train_y\", train_y)\n",
    "np.save(\"val_x\", val_x)\n",
    "np.save(\"val_y\", val_y)\n",
    "np.save(\"test_x\", test_x)\n",
    "np.save(\"test_y\", test_y)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset preprocessed\n",
    "train_x = np.load(\"train_x.npy\")\n",
    "train_y = np.load(\"train_y.npy\")\n",
    "val_x = np.load(\"val_x.npy\")\n",
    "val_y = np.load(\"val_y.npy\")\n",
    "test_x = np.load(\"test_x.npy\")\n",
    "test_y = np.load(\"test_y.npy\")\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "        \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "        \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "\n",
    "print(\"Train set: \\t\\t{}\".format(train_y.shape),\n",
    "        \"\\nValidation set: \\t{}\".format(val_y.shape),\n",
    "        \"\\nTest set: \\t\\t{}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estos valores habria que cambiarlo tambien arriba en caso necesario\n",
    "numwords = 200  # Limit the number of words to use\n",
    "seq_len = train_x.shape[1]\n",
    "NUM_OF_CLASSES = 2\n",
    "\n",
    "############################################\n",
    "# Model\n",
    "drop = 0.25\n",
    "nlayers = 1  # >= 1\n",
    "RNN = SimpleRNN\n",
    "\n",
    "neurons = 64\n",
    "embedding = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(numwords + 1, embedding, input_length=seq_len))\n",
    "\n",
    "if nlayers == 1:\n",
    "    model.add(RNN(neurons, implementation=impl, recurrent_dropout=drop))\n",
    "else:\n",
    "    model.add(RNN(neurons, implementation=impl, recurrent_dropout=drop, return_sequences=True))\n",
    "    for i in range(1, nlayers - 1):\n",
    "        model.add(RNN(neurons, recurrent_dropout=drop, implementation=impl, return_sequences=True))\n",
    "    model.add(RNN(neurons, recurrent_dropout=drop, implementation=impl))\n",
    "\n",
    "model.add(Dense(NUM_OF_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "############################################\n",
    "# Training\n",
    "\n",
    "learning_rate = 0.000001\n",
    "#optimizer = SGD(lr=learning_rate, momentum=0.95)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "#optimizer = Adamax(lr=learning_rate)\n",
    "#optimizer = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "#model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "#    tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://10.0.101.2:8470'))\n",
    "#)\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "train_y_c = np_utils.to_categorical(train_y, NUM_OF_CLASSES)\n",
    "val_y_c = np_utils.to_categorical(val_y, NUM_OF_CLASSES)\n",
    "\n",
    "# Callbacks\n",
    "callbacks_list = [\n",
    "    TensorBoard(log_dir=\"logs/{}\".format(time.time())),\n",
    "    EarlyStopping(monitor=\"val_acc\", patience=10),\n",
    "]\n",
    "\n",
    "model.fit(train_x, train_y_c,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(val_x, val_y_c),\n",
    "            verbose=verbose, callbacks=callbacks_list)\n",
    "\n",
    "############################################\n",
    "# Results\n",
    "\n",
    "test_y_c = np_utils.to_categorical(test_y, NUM_OF_CLASSES)\n",
    "score, acc = model.evaluate(test_x, test_y_c,\n",
    "                                batch_size=batch_size,\n",
    "                                verbose=verbose)\n",
    "print()\n",
    "print('Test ACC=', acc)\n",
    "\n",
    "'''test_pred = model.predict_classes(test_x, verbose=verbose)\n",
    "\n",
    "print()\n",
    "print('Confusion Matrix')\n",
    "print('-'*20)\n",
    "print(confusion_matrix(test_y, test_pred))\n",
    "print()\n",
    "print('Classification Report')\n",
    "print('-'*40)\n",
    "print(classification_report(test_y, test_pred))\n",
    "print()\n",
    "print(\"Ending:\", time.ctime())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Ending:\", time.ctime())\n",
    "\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(train_y)):\n",
    "    print(train_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
